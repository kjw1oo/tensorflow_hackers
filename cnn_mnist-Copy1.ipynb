{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./mnist/data/', one_hot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model/out/BiasAdd:0\", shape=(?, 10), dtype=float32, device=/device:GPU:0)\n",
      "INFO:tensorflow:Summary name Var_model/CONV1/kernel:0 is illegal; using Var_model/CONV1/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV1/kernel:0' shape=(3, 3, 1, 8) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV1/bias:0 is illegal; using Var_model/CONV1/bias_0 instead.\n",
      "<tf.Variable 'model/CONV1/bias:0' shape=(8,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/BN1/gamma:0 is illegal; using Var_model/BN1/gamma_0 instead.\n",
      "<tf.Variable 'model/BN1/gamma:0' shape=(8,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/BN1/beta:0 is illegal; using Var_model/BN1/beta_0 instead.\n",
      "<tf.Variable 'model/BN1/beta:0' shape=(8,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV2/kernel:0 is illegal; using Var_model/CONV2/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV2/kernel:0' shape=(3, 3, 8, 16) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV2/bias:0 is illegal; using Var_model/CONV2/bias_0 instead.\n",
      "<tf.Variable 'model/CONV2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/BN2/gamma:0 is illegal; using Var_model/BN2/gamma_0 instead.\n",
      "<tf.Variable 'model/BN2/gamma:0' shape=(16,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/BN2/beta:0 is illegal; using Var_model/BN2/beta_0 instead.\n",
      "<tf.Variable 'model/BN2/beta:0' shape=(16,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV3/kernel:0 is illegal; using Var_model/CONV3/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV3/kernel:0' shape=(3, 3, 16, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV3/bias:0 is illegal; using Var_model/CONV3/bias_0 instead.\n",
      "<tf.Variable 'model/CONV3/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/BN3/gamma:0 is illegal; using Var_model/BN3/gamma_0 instead.\n",
      "<tf.Variable 'model/BN3/gamma:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/BN3/beta:0 is illegal; using Var_model/BN3/beta_0 instead.\n",
      "<tf.Variable 'model/BN3/beta:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV4/kernel:0 is illegal; using Var_model/CONV4/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV4/kernel:0' shape=(3, 3, 32, 64) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV4/bias:0 is illegal; using Var_model/CONV4/bias_0 instead.\n",
      "<tf.Variable 'model/CONV4/bias:0' shape=(64,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/BN4/gamma:0 is illegal; using Var_model/BN4/gamma_0 instead.\n",
      "<tf.Variable 'model/BN4/gamma:0' shape=(64,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/BN4/beta:0 is illegal; using Var_model/BN4/beta_0 instead.\n",
      "<tf.Variable 'model/BN4/beta:0' shape=(64,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/out/kernel:0 is illegal; using Var_model/out/kernel_0 instead.\n",
      "<tf.Variable 'model/out/kernel:0' shape=(64, 10) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/out/bias:0 is illegal; using Var_model/out/bias_0 instead.\n",
      "<tf.Variable 'model/out/bias:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "#hyperparameter\n",
    "params = {\n",
    "    'num_classes': 10,\n",
    "    # cnn 파라미터\n",
    "    'use_cnn': True,\n",
    "    'num_filters': [8, 16, 32, 64],\n",
    "    'filter_size': [3, 3, 3, 3],\n",
    "    'cnn_batch_norm' : [True, True, True, True],\n",
    "    # dense 파라미터\n",
    "    'use_fc': False,\n",
    "    'fc_hidden_units': [1028, 512, 256],\n",
    "    'fc_batch_norm': [True, True, True],\n",
    "    'fc_dropout_keep_prob': [0.2, 0.3, 0.35],\n",
    "    # rnn(lstm) 파라미터\n",
    "    'use_rnn': False,\n",
    "    'rnn_n_hiddens': [1028],\n",
    "    'rnn_dropout_keep_prob': [0.6],\n",
    "    \n",
    "    # Global Average Pooling /RNN이랑 동시사용불가\n",
    "    'use_GAP' : True,\n",
    "    \n",
    "    'learning_rate': 0.01,\n",
    "    'activation' : tf.nn.relu,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 1,\n",
    "    'height': 126,\n",
    "    'width': 126,\n",
    "    'model_path': './model/first_cnn_LSTM(4_3)/' \n",
    "}\n",
    "class Model:\n",
    "    def __init__(self, params,name):\n",
    "        # 하이퍼파라미터\n",
    "        self.num_classes = params['num_classes']\n",
    "        \n",
    "        self.use_cnn = params['use_cnn']\n",
    "        self.num_filters = params['num_filters']\n",
    "        self.filter_sizes = params['filter_size']\n",
    "        self.cnn_batch_norm  = params['cnn_batch_norm']\n",
    "        \n",
    "        self.use_fc = params['use_fc']\n",
    "        self.fc_hidden_units = params['fc_hidden_units']\n",
    "        self.fc_batch_norm = params['fc_batch_norm']\n",
    "        self.fc_dropout_keep_prob = params['fc_dropout_keep_prob']\n",
    "        \n",
    "        self.use_rnn = params['use_rnn']\n",
    "        self.rnn_n_hiddens = params['rnn_n_hiddens']\n",
    "        self.rnn_dropout_keep_prob = params['rnn_dropout_keep_prob']\n",
    "        \n",
    "        self.use_GAP = params['use_GAP']\n",
    "        \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.activation = params['activation']\n",
    "        \n",
    "        self.height = params['height']\n",
    "        self.width = params['width']\n",
    "        self.model_path = params['model_path']\n",
    "        self.idx_convolutional_layers = range(1, len(self.filter_sizes) + 1)\n",
    "        self.idx_fc_layers = range(1, len(self.fc_hidden_units) + 1)\n",
    "        self.idx_rnn_layers = range(1, len(self.rnn_n_hiddens) + 1)\n",
    "        self.name = name\n",
    "        \n",
    "\n",
    "    #  컨볼루션 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def convolutional_layers(self, X, is_training = True, reuse = False):\n",
    "        \n",
    "        inputs = X\n",
    "        for i, num_filter, filter_size, use_bn in zip(self.idx_convolutional_layers,\n",
    "                                                      self.num_filters,\n",
    "                                                      self.filter_sizes,\n",
    "                                                      self.cnn_batch_norm):            \n",
    "            L = tf.layers.conv2d(inputs,\n",
    "                                 filters=num_filter,\n",
    "                                 kernel_size=filter_size,\n",
    "                                 strides=1,\n",
    "                                 padding='SAME',\n",
    "                                 name = 'CONV'+str(i),\n",
    "                                 reuse= reuse)\n",
    "            if use_bn:\n",
    "                L= tf.layers.batch_normalization(L, training= is_training, name='BN' + str(i), reuse= reuse)\n",
    "            L = self.activation(L)\n",
    "            L = tf.layers.max_pooling2d(L, pool_size = 2, strides = 2, padding = 'SAME')\n",
    "            inputs = L\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    #  dense 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def fc_layers(self, X, is_training = True, reuse = False):\n",
    "        inputs = X\n",
    "        for i, units, use_bn, keep_prob in zip(self.idx_fc_layers, self.fc_hidden_units, self.fc_batch_norm, self.fc_dropout_keep_prob):\n",
    "            fc = tf.layers.dense(inputs,\n",
    "                                 units=units,\n",
    "                                 reuse=reuse,\n",
    "                                 name = 'FC' + str(i))\n",
    "            if use_bn:\n",
    "                fc = tf.layers.batch_normalization(fc, training= is_training, name='fc_BN' + str(i), reuse= reuse)\n",
    "            fc = self.activation(fc)\n",
    "            if keep_prob:\n",
    "                fc = tf.layers.dropout(fc, rate = keep_prob, training= is_training, name = 'fc_dropout' + str(i))\n",
    "            inputs = fc \n",
    "        return inputs\n",
    "  \n",
    "\n",
    "     # LSTM 레이어 \n",
    "    def rnn_layers(self, inputs, is_training = True, reuse = False):\n",
    "        if is_training:\n",
    "            keep_probs = self.rnn_dropout_keep_prob\n",
    "            \n",
    "        else:\n",
    "            keep_probs = np.ones_like(self.rnn_dropout_keep_prob)\n",
    "            \n",
    "        # single layer\n",
    "        if len(self.idx_rnn_layers) == 1:\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_n_hiddens[0], reuse = reuse)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probs[0])\n",
    "        # multi layer \n",
    "        else:\n",
    "            cell_list = []\n",
    "            for i, n_hidden, keep_prob in zip(self.idx_rnn_layers, self.rnn_n_hiddens, keep_probs):\n",
    "                cell_ = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, reuse = reuse)\n",
    "                cell_ = tf.nn.rnn_cell.DropoutWrapper(cell_, output_keep_prob=keep_prob)\n",
    "                cell_list.append(cell_)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        # output_shape [batch_size, width(n_step), n_classes]\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        print(outputs.get_shape().as_list())\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        return outputs\n",
    " \n",
    "\n",
    "    def get_reshaped_cnn_to_rnn(self, inputs):\n",
    "        # [batch, height, width, n_feature map]\n",
    "        shape = inputs.get_shape().as_list() \n",
    "        # 우리가 얻어야하는 사이즈 [batch, width, height x n_feature map]\n",
    "        inputs = tf.transpose(inputs, [0, 2, 1, 3])\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, shape[2], shape[1] * shape[3]])\n",
    "        return reshaped_inputs\n",
    "  \n",
    "\n",
    "\n",
    "    # 모델 구축/ logit \n",
    "    def get_logits(self, X, is_training = True, reuse = False):\n",
    "        with tf.variable_scope(self.name):\n",
    "            L = X\n",
    "            if self.use_cnn:\n",
    "                L = self.convolutional_layers(L, is_training, reuse)\n",
    "\n",
    "            if self.use_rnn:\n",
    "                reshaped_fp = self.get_reshaped_cnn_to_rnn(L)\n",
    "                L = self.rnn_layers(reshaped_fp, is_training, reuse)\n",
    "                \n",
    "            if self.use_GAP:\n",
    "                shape =L.get_shape().as_list()\n",
    "                # 글로벌 풀링 사이즈 (height, width)\n",
    "                pool_size = (shape[1], shape[2])\n",
    "                L= tf.layers.average_pooling2d(L, pool_size = pool_size, strides = 1, padding = 'VALID')\n",
    "                # 마지막 dense layer를 위한 flatten\n",
    "                L = tf.layers.flatten(L)\n",
    "            if self.use_fc:\n",
    "                if not self.use_GAP:\n",
    "                    flat = tf.layers.flatten(L)\n",
    "                L = self.fc_layers(flat, is_training, reuse)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            output = tf.layers.dense(L, units= self.num_classes, reuse=reuse, name = 'out')\n",
    "            return output\n",
    "\n",
    "name = 'model'\n",
    "model = Model(params, 'model')\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    Y = tf.placeholder(tf.float32, [None, 10])\n",
    "    global_step = tf.Variable(0, trainable = False, name = 'global_step')\n",
    "\n",
    "    logits_train = model.get_logits(X)     \n",
    "    print(logits_train)\n",
    "    loss = tf.losses.softmax_cross_entropy(Y, logits_train)   \n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)            \n",
    "    with tf.control_dependencies(update_ops):    \n",
    "        optimizer = tf.train.AdamOptimizer(params['learning_rate']).minimize(loss, global_step=global_step)\n",
    "    #eval\n",
    "    logits_eval = model.get_logits(X, is_training = False, reuse = True)\n",
    "    predict_proba_ = tf.nn.softmax(logits_eval)\n",
    "    prediction = tf.argmax(predict_proba_, 1)\n",
    "    accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), prediction)\n",
    "    #predict\n",
    "    logits_test = model.get_logits(X, is_training = False, reuse = True)\n",
    "    test_predict_proba_ = tf.nn.softmax(logits_test)\n",
    "    test_prediction = tf.argmax(test_predict_proba_, 1)\n",
    "    # 변수들 프린트/ 텐서보드 summary 생성            \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    for v in tf.trainable_variables():\n",
    "        tf.summary.histogram('Var_{}'.format(v.name), v)\n",
    "        print(v)            \n",
    "    merged = tf.summary.merge_all()\n",
    "# 모델저장\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, cost : 0.10082119703292847, acc: (0.80707943, 0.83015269)\n",
      "epoch : 1, cost : 0.06959754228591919, acc: (0.89179003, 0.89818549)\n",
      "epoch : 2, cost : 0.003349574049934745, acc: (0.92077458, 0.92380774)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-451cb0a83898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_summ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerged\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_summ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,gpu_options=gpu_options))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
    "        \n",
    "total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_cost = 0\n",
    "        \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, _summ = sess.run([optimizer, loss, merged], feed_dict = {X:batch_xs.reshape(-1, 28, 28, 1), Y: batch_ys})\n",
    "        writer.add_summary(_summ, i)\n",
    "    acc = sess.run(accuracy, feed_dict = {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "            \n",
    "    print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
    "for epoch in range(10):\n",
    "    total_cost = 0\n",
    "        \n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        _, c, _summ = sess.run([optimizer, loss, merged], feed_dict = {X:batch_xs.reshape(-1, 28, 28, 1), Y: batch_ys})\n",
    "        writer.add_summary(_summ, i)\n",
    "    acc = sess.run(accuracy, feed_dict = {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "            \n",
    "    print('epoch : {}, cost : {}, acc: {}'.format(epoch, c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameter\n",
    "params = {\n",
    "    'num_classes': 10,\n",
    "    \n",
    "    'num_filters': [8, 16, 32],\n",
    "    'filter_size': [2, 2, 2],\n",
    "    'cnn_batch_norm' : [False, False, False],\n",
    "    \n",
    "    'fc_hidden_units': [256, 128],\n",
    "    'fc_batch_norm': [False, False],\n",
    "    'fc_dropout_keep_prob': [0.2, 0.3],\n",
    "    \n",
    "    'rnn_n_step': 28, #width, time\n",
    "    'rnn_n_hiddens': [128, 128],\n",
    "    'rnn_dropout_keep_prob': [0.5, 0.6],\n",
    "    \n",
    "    'learning_rate': 0.01,\n",
    "    'activation' : tf.nn.relu,\n",
    "    'batch_size': 100\n",
    "}\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, params, sess, name):\n",
    "        # 하이퍼파라미터\n",
    "        self.num_classes = params['num_classes']\n",
    "        self.num_filters = params['num_filters']\n",
    "        self.filter_sizes = params['filter_size']\n",
    "        self.cnn_batch_norm  = params['cnn_batch_norm']\n",
    "        \n",
    "        self.fc_hidden_units = params['fc_hidden_units']\n",
    "        self.fc_batch_norm = params['fc_batch_norm']\n",
    "        self.fc_dropout_keep_prob = params['fc_dropout_keep_prob']\n",
    "        \n",
    "        self.rnn_n_hiddens = params['rnn_n_hiddens']\n",
    "        self.rnn_dropout_keep_prob = params['rnn_dropout_keep_prob']\n",
    "        \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.activation = params['activation']\n",
    "        self.batch_size = params['batch_size']        \n",
    "        \n",
    "        self.idx_convolutional_layers = range(1, len(self.filter_sizes) + 1)\n",
    "        self.idx_fc_layers = range(1, len(self.fc_hidden_units) + 1)\n",
    "        self.idx_rnn_layers = range(1, len(self.rnn_n_hiddens) + 1)\n",
    "        self.name = name\n",
    "        self.sess = sess\n",
    "        # 플레이스홀더\n",
    "        with tf.device('/gpu:0'):\n",
    "            self.X = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"input_x\")\n",
    "            self.Y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"input_y\")\n",
    "            \n",
    "\n",
    "            self._build_net()\n",
    "\n",
    "        \n",
    "        \n",
    "    #  컨볼루션 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def convolutional_layers(self, X, is_training = True, reuse = False):\n",
    "        inputs = X\n",
    "        for i, num_filter, filter_size, use_bn in zip(self.idx_convolutional_layers, self.num_filters, self.filter_sizes, self.cnn_batch_norm):            \n",
    "            L = tf.layers.conv2d(inputs,\n",
    "                                 filters=num_filter,\n",
    "                                 kernel_size=filter_size,\n",
    "                                 strides=1,\n",
    "                                 padding='SAME',\n",
    "                                 name = 'CONV'+str(i),\n",
    "                                 reuse= reuse)\n",
    "            if use_bn:\n",
    "                L= tf.layers.batch_normalization(L, training= is_training, name='BN' + str(i), reuse= reuse)\n",
    "            L = self.activation(L)\n",
    "            L = tf.layers.max_pooling2d(L, pool_size = 2, strides = 2, padding = 'SAME')\n",
    "            inputs = L\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    #  dense 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def fc_layers(self, X, is_training = True, reuse = False):\n",
    "        inputs = X\n",
    "        for i, units, use_bn, keep_prob in zip(self.idx_fc_layers, self.fc_hidden_units, self.fc_batch_norm, self.fc_dropout_keep_prob):\n",
    "            fc = tf.layers.dense(inputs,\n",
    "                                 units=units,\n",
    "                                 reuse=reuse,\n",
    "                                 name = 'FC' + str(i))\n",
    "            if use_bn:\n",
    "                fc = tf.layers.batch_normalization(fc, training= is_training, name='fc_BN' + str(i), reuse= reuse)\n",
    "            fc = self.activation(fc)\n",
    "            if keep_prob:\n",
    "                fc = tf.layers.dropout(fc, rate = keep_prob, training= is_training, name = 'fc_dropout' + str(i))\n",
    "            inputs = fc \n",
    "        return inputs\n",
    "  \n",
    "\n",
    "     # LSTM 레이어 \n",
    "    def rnn_layers(self, inputs, is_training = True, reuse = False):\n",
    "        if is_training:\n",
    "            keep_probs = self.rnn_dropout_keep_prob\n",
    "            \n",
    "        else:\n",
    "            keep_probs = np.ones_like(self.rnn_dropout_keep_prob)\n",
    "        # single layer\n",
    "        if len(self.idx_rnn_layers) == 1:\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_n_hiddens[0], reuse = reuse)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probs[0])\n",
    "        # multi layer \n",
    "        else:\n",
    "            cell_list = []\n",
    "            for i, n_hidden, keep_prob in zip(self.idx_rnn_layers, self.rnn_n_hiddens, keep_probs):\n",
    "                cell_ = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, reuse = reuse)\n",
    "                cell_ = tf.nn.rnn_cell.DropoutWrapper(cell_, output_keep_prob=keep_prob)\n",
    "                cell_list.append(cell_)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        # output_shape [batch_size, width(n_step), n_classes]\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        print(outputs.get_shape().as_list())\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        return outputs\n",
    " \n",
    "\n",
    "    def get_reshaped_cnn_to_rnn(self, inputs):\n",
    "        # [batch, height, width, n_feature map]\n",
    "        shape = inputs.get_shape().as_list() \n",
    "        # 우리가 얻어야하는 사이즈 [batch, height, width x n_feature map]\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, shape[1], shape[2] * shape[3]])\n",
    "        return reshaped_inputs\n",
    "  \n",
    "\n",
    "\n",
    "    # 모델 구축/ logit \n",
    "    def get_logits(self, X, is_training = True, reuse = False):        \n",
    "        conv = self.convolutional_layers(self.X, is_training, reuse)                           \n",
    "        #flat = tf.layers.flatten(conv)\n",
    "        reshaped_fp = self.get_reshaped_cnn_to_rnn(conv)\n",
    "        rnn = self.rnn_layers(reshaped_fp, is_training, reuse)\n",
    "        #fc = self.fc_layers(flat, is_training, reuse)\n",
    "        output = tf.layers.dense(rnn, units= self.num_classes, reuse=reuse, name = 'out')\n",
    "        \n",
    "        return output\n",
    " \n",
    "\n",
    "\n",
    "    # 모델 구축\n",
    "    def _build_net(self):\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "            self.logits_train = self.get_logits(self.X)                              \n",
    "            self.loss = tf.losses.softmax_cross_entropy(self.Y, self.logits_train)   \n",
    "                # batch_normalization 적용을 위해 모든 변수들을 불러와서 moving\n",
    "                #학습 단계에서는 데이터가 배치 단위로 들어오기 때문에 배치의 평균, 분산을 구하는 것이 가능하지만,\n",
    "                # 테스트 단계에서는 배치 단위로 평균/분산을 구하기가 어렵기때문에\n",
    "                # 학습 단계에서 배치 단위의 평균/분산을 저장해 놓고 테스트 시에는 평균/분산을 사용합니다.\n",
    "                # 저장한 값을 get_collection을 통해서 불러온다.\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)            \n",
    "            with tf.control_dependencies(update_ops):    \n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "            self.logits_eval = self.get_logits(self.X, is_training = False, reuse = True)\n",
    "            self.predict_proba_ = tf.nn.softmax(self.logits_eval)\n",
    "            self.prediction = tf.argmax(self.predict_proba_, 1)\n",
    "            #self.accuracy = tf.metrics.accuracy(tf.argmax(self.Y, 1), self.prediction)\n",
    "            # 변수들 프린트/ 텐서보드 summary 생성\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        # tf.summary.scalar('accuracy', self.accuracy[1])\n",
    "        for v in tf.trainable_variables():\n",
    "            tf.summary.histogram('Var_{}'.format(v.name), v)\n",
    "            print(v)            \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        # 모델저장\n",
    "        saver = tf.train.Saver()\n",
    "            \n",
    "        \n",
    "    def fit(self):\n",
    "        total_batch = int(mnist.train.num_examples/ self.batch_size)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
    "        for epoch in range(10):\n",
    "            total_cost = 0\n",
    "        \n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(self.batch_size)\n",
    "                _, c, _summ = sess.run([self.optimizer, self.loss, self.merged], feed_dict = {self.X:batch_xs.reshape(-1, 28, 28, 1), self.Y: batch_ys})\n",
    "                writer.add_summary(_summ, i)\n",
    "            #acc = sess.run(self.accuracy, feed_dict = {self.X: mnist.test.images.reshape(-1, 28, 28, 1), self.Y: mnist.test.labels})\n",
    "            \n",
    "            print('epoch : {}, cost : {}, acc: {}'.format(epoch, c))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 4, 128]\n",
      "[None, 4, 128]\n",
      "INFO:tensorflow:Summary name Var_model/CONV1/kernel:0 is illegal; using Var_model/CONV1/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV1/kernel:0' shape=(2, 2, 1, 8) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV1/bias:0 is illegal; using Var_model/CONV1/bias_0 instead.\n",
      "<tf.Variable 'model/CONV1/bias:0' shape=(8,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV2/kernel:0 is illegal; using Var_model/CONV2/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV2/kernel:0' shape=(2, 2, 8, 16) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV2/bias:0 is illegal; using Var_model/CONV2/bias_0 instead.\n",
      "<tf.Variable 'model/CONV2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV3/kernel:0 is illegal; using Var_model/CONV3/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV3/kernel:0' shape=(2, 2, 16, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV3/bias:0 is illegal; using Var_model/CONV3/bias_0 instead.\n",
      "<tf.Variable 'model/CONV3/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0 is illegal; using Var_model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0 instead.\n",
      "<tf.Variable 'model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0 is illegal; using Var_model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0 instead.\n",
      "<tf.Variable 'model/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0 is illegal; using Var_model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0 instead.\n",
      "<tf.Variable 'model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0 is illegal; using Var_model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0 instead.\n",
      "<tf.Variable 'model/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/out/kernel:0 is illegal; using Var_model/out/kernel_0 instead.\n",
      "<tf.Variable 'model/out/kernel:0' shape=(128, 10) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/out/bias:0 is illegal; using Var_model/out/bias_0 instead.\n",
      "<tf.Variable 'model/out/bias:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model(params, sess, name = 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot assign a device for operation 'model/rnn_1/range': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: model/rnn_1/range = Range[Tidx=DT_INT32, _device=\"/device:GPU:0\"](model/rnn_1/range/start, model/rnn_1/Rank, model/rnn_1/range/delta)]]\n\nCaused by op 'model/rnn_1/range', defined at:\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-8becbd984eae>\", line 2, in <module>\n    model = Model(params, sess, name = 'model')\n  File \"<ipython-input-11-f9b1783ff22b>\", line 52, in __init__\n    self._build_net()\n  File \"<ipython-input-11-f9b1783ff22b>\", line 157, in _build_net\n    self.logits_eval = self.get_logits(self.X, is_training = False, reuse = True)\n  File \"<ipython-input-11-f9b1783ff22b>\", line 133, in get_logits\n    rnn = self.rnn_layers(reshaped_fp, is_training, reuse)\n  File \"<ipython-input-11-f9b1783ff22b>\", line 112, in rnn_layers\n    outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 621, in dynamic_rnn\n    outputs = nest.map_structure(_transpose_batch_time, outputs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 413, in map_structure\n    structure[0], [func(*x) for x in entries])\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 413, in <listcomp>\n    structure[0], [func(*x) for x in entries])\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 71, in _transpose_batch_time\n    ([1, 0], math_ops.range(2, x_rank)), axis=0))\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1234, in range\n    return gen_math_ops._range(start, limit, delta, name=name)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3258, in _range\n    \"Range\", start=start, limit=limit, delta=delta, name=name)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'model/rnn_1/range': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: model/rnn_1/range = Range[Tidx=DT_INT32, _device=\"/device:GPU:0\"](model/rnn_1/range/start, model/rnn_1/Rank, model/rnn_1/range/delta)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1292\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1353\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1354\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'model/rnn_1/range': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: model/rnn_1/range = Range[Tidx=DT_INT32, _device=\"/device:GPU:0\"](model/rnn_1/range/start, model/rnn_1/Rank, model/rnn_1/range/delta)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fed1f18ffa7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-f9b1783ff22b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mtotal_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_examples\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mwriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./logs/'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot assign a device for operation 'model/rnn_1/range': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: model/rnn_1/range = Range[Tidx=DT_INT32, _device=\"/device:GPU:0\"](model/rnn_1/range/start, model/rnn_1/Rank, model/rnn_1/range/delta)]]\n\nCaused by op 'model/rnn_1/range', defined at:\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-8becbd984eae>\", line 2, in <module>\n    model = Model(params, sess, name = 'model')\n  File \"<ipython-input-11-f9b1783ff22b>\", line 52, in __init__\n    self._build_net()\n  File \"<ipython-input-11-f9b1783ff22b>\", line 157, in _build_net\n    self.logits_eval = self.get_logits(self.X, is_training = False, reuse = True)\n  File \"<ipython-input-11-f9b1783ff22b>\", line 133, in get_logits\n    rnn = self.rnn_layers(reshaped_fp, is_training, reuse)\n  File \"<ipython-input-11-f9b1783ff22b>\", line 112, in rnn_layers\n    outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 621, in dynamic_rnn\n    outputs = nest.map_structure(_transpose_batch_time, outputs)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 413, in map_structure\n    structure[0], [func(*x) for x in entries])\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\", line 413, in <listcomp>\n    structure[0], [func(*x) for x in entries])\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 71, in _transpose_batch_time\n    ([1, 0], math_ops.range(2, x_rank)), axis=0))\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 1234, in range\n    return gen_math_ops._range(start, limit, delta, name=name)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 3258, in _range\n    \"Range\", start=start, limit=limit, delta=delta, name=name)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'model/rnn_1/range': Operation was explicitly assigned to /device:GPU:0 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device.\n\t [[Node: model/rnn_1/range = Range[Tidx=DT_INT32, _device=\"/device:GPU:0\"](model/rnn_1/range/start, model/rnn_1/Rank, model/rnn_1/range/delta)]]\n"
     ]
    }
   ],
   "source": [
    "model.fit()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22.  28.]\n",
      " [ 49.  64.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(\n",
    "      allow_soft_placement=True, log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, cost : 0.053370747715234756, acc: [(0.0, 0.96810001)]\n",
      "epoch : 1, cost : 0.06267564743757248, acc: [(0.96810001, 0.97430003)]\n",
      "epoch : 2, cost : 0.0855538621544838, acc: [(0.97430003, 0.97680002)]\n",
      "epoch : 3, cost : 0.19877442717552185, acc: [(0.97680002, 0.97907501)]\n",
      "epoch : 4, cost : 0.05799204856157303, acc: [(0.97907501, 0.98093998)]\n",
      "epoch : 5, cost : 0.11063935607671738, acc: [(0.98093998, 0.98213333)]\n",
      "epoch : 6, cost : 0.03810318559408188, acc: [(0.98213333, 0.98282856)]\n",
      "epoch : 7, cost : 0.0006602299981750548, acc: [(0.98282856, 0.98348749)]\n",
      "epoch : 8, cost : 0.033298783004283905, acc: [(0.98348749, 0.98413336)]\n",
      "epoch : 9, cost : 0.008241044357419014, acc: [(0.98413336, 0.98462999)]\n",
      "epoch : 10, cost : 0.02387131005525589, acc: [(0.98462999, 0.98500907)]\n",
      "epoch : 11, cost : 0.0030551606323570013, acc: [(0.98500907, 0.98540002)]\n",
      "epoch : 12, cost : 0.06204137206077576, acc: [(0.98540002, 0.98581541)]\n",
      "epoch : 13, cost : 0.0004125334962736815, acc: [(0.98581541, 0.98612142)]\n",
      "epoch : 14, cost : 0.023427624255418777, acc: [(0.98612142, 0.98637998)]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev = 0.01))\n",
    "L1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev = 0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "L2_flatten = tf.contrib.layers.flatten(L2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([7 * 7 * 64, 256], stddev = 0.01))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2_flatten, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10], stddev =0.01))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L3, W4) + b4\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(Y, logits = logits)\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "predictions = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), predictions)\n",
    "\n",
    "batch_size = 100\n",
    "with tf.Session() as sess:\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for epoch in range(15):\n",
    "        total_cost = 0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c, = sess.run([train, cost], feed_dict = {X:batch_xs.reshape(-1, 28, 28, 1), Y: batch_ys, keep_prob: 0.8})\n",
    "        acc = sess.run([accuracy], feed_dict = {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels, keep_prob: 1.0})\n",
    "        print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def model_net(x, activation, is_training, reuse = False):\n",
    "    L1 = tf.layers.conv2d(x, 32, 3, padding='SAME', activation = activation, reuse= reuse, name = 'L1')\n",
    "    L1 = tf.layers.max_pooling2d(L1, 2, 2)\n",
    "    L2 = tf.layers.conv2d(L1, 64, 3, padding='SAME', activation = activation, reuse=reuse, name = 'L2')\n",
    "    L2 = tf.layers.max_pooling2d(L2, 2, 2)\n",
    "    \n",
    "    L2_flatten = tf.contrib.layers.flatten(L2)\n",
    "    \n",
    "    fc1 = tf.layers.dense(L2_flatten, 256, activation = activation, reuse=reuse, name = 'FC1')\n",
    "    fc1 = tf.layers.dropout(fc1, 0.2, training = is_training)\n",
    "    \n",
    "    fc2 = tf.layers.dense(fc1, 10, reuse=reuse, name = 'output')\n",
    "    return fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = model_net(X, tf.nn.relu, True)\n",
    "test_logits = model_net(X, tf.nn.relu, False, True)\n",
    "cost = tf.losses.softmax_cross_entropy(Y, logits = logits)\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "predictions = tf.argmax(tf.nn.softmax(test_logits), 1)\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, cost : 0.020873257890343666, acc: [(0.0, 0.98430002)]\n",
      "epoch : 1, cost : 0.012488684616982937, acc: [(0.98430002, 0.98689997)]\n",
      "epoch : 2, cost : 0.058932267129421234, acc: [(0.98689997, 0.9878)]\n",
      "epoch : 3, cost : 0.050051361322402954, acc: [(0.9878, 0.98860002)]\n",
      "epoch : 4, cost : 0.047780223190784454, acc: [(0.98860002, 0.98923999)]\n",
      "epoch : 5, cost : 0.03912124037742615, acc: [(0.98923999, 0.98943335)]\n",
      "epoch : 6, cost : 0.02453668788075447, acc: [(0.98943335, 0.98968571)]\n",
      "epoch : 7, cost : 0.0036200848408043385, acc: [(0.98968571, 0.98982501)]\n",
      "epoch : 8, cost : 0.0011123416479676962, acc: [(0.98982501, 0.99014443)]\n",
      "epoch : 9, cost : 0.004260535817593336, acc: [(0.99014443, 0.98979002)]\n",
      "epoch : 10, cost : 0.007146528456360102, acc: [(0.98979002, 0.98985457)]\n",
      "epoch : 11, cost : 0.04281054064631462, acc: [(0.98985457, 0.99010831)]\n",
      "epoch : 12, cost : 0.0008145206375047565, acc: [(0.99010831, 0.99017692)]\n",
      "epoch : 13, cost : 0.02385837584733963, acc: [(0.99017692, 0.99017859)]\n",
      "epoch : 14, cost : 0.00028013234259560704, acc: [(0.99017859, 0.99027997)]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "with tf.Session() as sess:\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for epoch in range(15):\n",
    "        total_cost = 0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c, = sess.run([train, cost], feed_dict = {X:batch_xs.reshape(-1, 28, 28, 1), Y: batch_ys})\n",
    "        acc = sess.run([accuracy], feed_dict = {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "        print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))\n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def model_net(x, activation, is_training, reuse = False):\n",
    "    L1 = tf.layers.conv2d(x, 32, 3, padding='SAME', reuse= reuse, name = 'L1')\n",
    "    L1 = tf.layers.batch_normalization(L1, training=is_training)\n",
    "    L1 = activation(L1)\n",
    "    L1 = tf.layers.max_pooling2d(L1, 2, 2)\n",
    "    \n",
    "    L2 = tf.layers.conv2d(L1, 64, 3, padding='SAME', activation = activation, reuse=reuse, name = 'L2')\n",
    "    L2 = tf.layers.batch_normalization(L2, training=is_training)\n",
    "    L2 = activation(L2)\n",
    "    L2 = tf.layers.max_pooling2d(L2, 2, 2)\n",
    "    \n",
    "    L2_flatten = tf.contrib.layers.flatten(L2)\n",
    "    \n",
    "    fc1 = tf.layers.dense(L2_flatten, 256, activation = activation, reuse=reuse, name = 'FC1')\n",
    "    fc1 = tf.layers.dropout(fc1, 0.2, training = is_training)\n",
    "    \n",
    "    fc2 = tf.layers.dense(fc1, 10, reuse=reuse, name = 'output')\n",
    "    return fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = model_net(X, tf.nn.relu, True)\n",
    "test_logits = model_net(X, tf.nn.relu, False, True)\n",
    "cost = tf.losses.softmax_cross_entropy(Y, logits = logits)\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "predictions = tf.argmax(tf.nn.softmax(test_logits), 1)\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "with tf.Session() as sess:\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for epoch in range(15):\n",
    "        total_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c, = sess.run([train, cost], feed_dict = {X:batch_xs.reshape(-1, 28, 28, 1), Y: batch_ys})\n",
    "        acc = sess.run([accuracy], feed_dict = {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "        print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
