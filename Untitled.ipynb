{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, params,name):\n",
    "        # 하이퍼파라미터\n",
    "        self.num_classes = params['num_classes']\n",
    "        \n",
    "        self.use_cnn = params['use_cnn']\n",
    "        self.num_filters = params['num_filters']\n",
    "        self.filter_sizes = params['filter_size']\n",
    "        self.cnn_batch_norm  = params['cnn_batch_norm']\n",
    "        \n",
    "        self.use_fc = params['use_fc']\n",
    "        self.fc_hidden_units = params['fc_hidden_units']\n",
    "        self.fc_batch_norm = params['fc_batch_norm']\n",
    "        self.fc_dropout_keep_prob = params['fc_dropout_keep_prob']\n",
    "        \n",
    "        self.use_rnn = params['use_rnn']\n",
    "        self.rnn_n_hiddens = params['rnn_n_hiddens']\n",
    "        self.rnn_dropout_keep_prob = params['rnn_dropout_keep_prob']\n",
    "        \n",
    "        self.use_GAP = params['use_GAP']\n",
    "        \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.activation = params['activation']\n",
    "        \n",
    "        self.height = params['height']\n",
    "        self.width = params['width']\n",
    "        self.model_path = params['model_path']\n",
    "        self.idx_convolutional_layers = range(1, len(self.filter_sizes) + 1)\n",
    "        self.idx_fc_layers = range(1, len(self.fc_hidden_units) + 1)\n",
    "        self.idx_rnn_layers = range(1, len(self.rnn_n_hiddens) + 1)\n",
    "        self.name = name\n",
    "        \n",
    "\n",
    "    #  컨볼루션 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def convolutional_layers(self, X, is_training = True, reuse = False):\n",
    "        \n",
    "        inputs = X\n",
    "        for i, num_filter, filter_size, use_bn in zip(self.idx_convolutional_layers,\n",
    "                                                      self.num_filters,\n",
    "                                                      self.filter_sizes,\n",
    "                                                      self.cnn_batch_norm):            \n",
    "            L = tf.layers.conv2d(inputs,\n",
    "                                 filters=num_filter,\n",
    "                                 kernel_size=filter_size,\n",
    "                                 strides=1,\n",
    "                                 padding='SAME',\n",
    "                                 name = 'CONV'+str(i),\n",
    "                                 reuse= reuse)\n",
    "            if use_bn:\n",
    "                L= tf.layers.batch_normalization(L, training= is_training, name='BN' + str(i), reuse= reuse)\n",
    "            L = self.activation(L)\n",
    "            L = tf.layers.max_pooling2d(L, pool_size = 2, strides = 2, padding = 'SAME')\n",
    "            inputs = L\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    #  dense 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def fc_layers(self, X, is_training = True, reuse = False):\n",
    "        inputs = X\n",
    "        for i, units, use_bn, keep_prob in zip(self.idx_fc_layers, self.fc_hidden_units, self.fc_batch_norm, self.fc_dropout_keep_prob):\n",
    "            fc = tf.layers.dense(inputs,\n",
    "                                 units=units,\n",
    "                                 reuse=reuse,\n",
    "                                 name = 'FC' + str(i))\n",
    "            if use_bn:\n",
    "                fc = tf.layers.batch_normalization(fc, training= is_training, name='fc_BN' + str(i), reuse= reuse)\n",
    "            fc = self.activation(fc)\n",
    "            if keep_prob:\n",
    "                fc = tf.layers.dropout(fc, rate = keep_prob, training= is_training, name = 'fc_dropout' + str(i))\n",
    "            inputs = fc \n",
    "        return inputs\n",
    "  \n",
    "\n",
    "     # LSTM 레이어 \n",
    "    def rnn_layers(self, inputs, is_training = True, reuse = False):\n",
    "        if is_training:\n",
    "            keep_probs = self.rnn_dropout_keep_prob\n",
    "            \n",
    "        else:\n",
    "            keep_probs = np.ones_like(self.rnn_dropout_keep_prob)\n",
    "            \n",
    "        # single layer\n",
    "        if len(self.idx_rnn_layers) == 1:\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_n_hiddens[0], reuse = reuse)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probs[0])\n",
    "        # multi layer \n",
    "        else:\n",
    "            cell_list = []\n",
    "            for i, n_hidden, keep_prob in zip(self.idx_rnn_layers, self.rnn_n_hiddens, keep_probs):\n",
    "                cell_ = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, reuse = reuse)\n",
    "                cell_ = tf.nn.rnn_cell.DropoutWrapper(cell_, output_keep_prob=keep_prob)\n",
    "                cell_list.append(cell_)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        # output_shape [batch_size, width(n_step), n_classes]\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        print(outputs.get_shape().as_list())\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        return outputs\n",
    " \n",
    "\n",
    "    def get_reshaped_cnn_to_rnn(self, inputs):\n",
    "        # [batch, height, width, n_feature map]\n",
    "        shape = inputs.get_shape().as_list() \n",
    "        # 우리가 얻어야하는 사이즈 [batch, width, height x n_feature map]\n",
    "        inputs = tf.transpose(inputs, [0, 2, 1, 3])\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, shape[2], shape[1] * shape[3]])\n",
    "        return reshaped_inputs\n",
    "  \n",
    "\n",
    "\n",
    "    # 모델 구축/ logit \n",
    "    def get_logits(self, X, is_training = True, reuse = False):\n",
    "        with tf.variable_scope(self.name):\n",
    "            L = X\n",
    "            if self.use_cnn:\n",
    "                L = self.convolutional_layers(L, is_training, reuse)\n",
    "\n",
    "            if self.use_rnn:\n",
    "                reshaped_fp = self.get_reshaped_cnn_to_rnn(L)\n",
    "                L = self.rnn_layers(reshaped_fp, is_training, reuse)\n",
    "                \n",
    "            if self.use_GAP:\n",
    "                shape =L.get_shape().as_list()\n",
    "                # 글로벌 풀링 사이즈 (height, width)\n",
    "                pool_size = (shape[1], shape[2])\n",
    "                L= tf.layers.average_pooling2d(L, pool_size = pool_size, strides = 1, padding = 'VALID')\n",
    "                # 마지막 dense layer를 위한 flatten\n",
    "                L = tf.layers.flatten(L)\n",
    "                \n",
    "            if self.use_fc:\n",
    "                if not self.use_GAP:\n",
    "                    flat = tf.layers.flatten(L)\n",
    "                L = self.fc_layers(flat, is_training, reuse)\n",
    "            \n",
    "                       \n",
    "            output = tf.layers.dense(L, units= self.num_classes, reuse=reuse, name = 'out')\n",
    "            return output\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
