{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./mnist/data/', one_hot = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n"
     ]
    }
   ],
   "source": [
    "for i, j, k in zip(range(1, 4), [4, 5, 6], [7, 8, 9]):\n",
    "    print(i, j ,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameter\n",
    "params = {\n",
    "    'num_classes': 10,\n",
    "    'num_filters': [8, 16, 32],\n",
    "    'filter_size': [2, 2, 2],\n",
    "    'activation' : tf.nn.relu,\n",
    "    'cnn_batch_norm' : [False, False, False],\n",
    "    'fc_hidden_units': [256, 128],\n",
    "    'fc_batch_norm': [False, False],\n",
    "    \n",
    "    'rnn_n_step': 28, #width, time\n",
    "    'rnn_n_hiddens': [128, 128],\n",
    "    'rnn_dropout_keep_prob': [0.5, 0.4],\n",
    "    \n",
    "    'learning_rate': 0.01,\n",
    "    \n",
    "    'batch_size': 100\n",
    "}\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, params, sess, name):\n",
    "        # 하이퍼파라미터\n",
    "        self.num_classes = params['num_classes']\n",
    "        self.num_filters = params['num_filters']\n",
    "        self.filter_sizes = params['filter_size']\n",
    "        self.cnn_batch_norm  = params['cnn_batch_norm']\n",
    "        \n",
    "        self.fc_hidden_units = params['fc_hidden_units']\n",
    "        self.fc_batch_norm = params['fc_batch_norm']\n",
    "        \n",
    "        self.rnn_n_hiddens = params['rnn_n_hiddens']\n",
    "        self.rnn_dropout_keep_prob = params['rnn_dropout_keep_prob']\n",
    "        \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.activation = params['activation']\n",
    "        self.batch_size = params['batch_size']        \n",
    "        \n",
    "        self.idx_convolutional_layers = range(1, len(self.filter_sizes) + 1)\n",
    "        self.idx_fc_layers = range(1, len(self.fc_hidden_units) + 1)\n",
    "        self.idx_rnn_layers = range(1, len(self.rnn_n_hiddens) + 1)\n",
    "        self.name = name\n",
    "        # 플레이스홀더\n",
    "        self.X = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"input_x\")\n",
    "        self.Y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"input_y\")\n",
    "        self.sess = sess\n",
    "        \n",
    "        self._build_net()\n",
    "\n",
    "    #  컨볼루션 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def convolutional_layers(self, X, is_training = True, reuse = False):\n",
    "        inputs = X\n",
    "        for i, num_filter, filter_size, use_bn in zip(self.idx_convolutional_layers, self.num_filters, self.filter_sizes, self.cnn_batch_norm):            \n",
    "            L = tf.layers.conv2d(inputs,\n",
    "                                 filters=num_filter,\n",
    "                                 kernel_size=filter_size,\n",
    "                                 strides=1,\n",
    "                                 padding='SAME',\n",
    "                                 name = 'CONV'+str(i),\n",
    "                                 reuse= reuse)\n",
    "            if use_bn:\n",
    "                L= tf.layers.batch_normalization(L, training= is_training, name='BN' + str(i), reuse= reuse)\n",
    "            L = self.activation(L)\n",
    "            L = tf.layers.max_pooling2d(L, pool_size = 2, strides = 2, padding = 'SAME')\n",
    "            inputs = L\n",
    "        return inputs\n",
    "    #  dense 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def fc_layers(self, X, is_training = True, reuse = False):\n",
    "        inputs = X\n",
    "        for i, units, use_bn in zip(self.idx_fc_layers, self.fc_hidden_units, self.fc_batch_norm):\n",
    "            fc = tf.layers.dense(inputs,\n",
    "                                 units=units,\n",
    "                                 reuse=reuse,\n",
    "                                 name = 'FC' + str(i))\n",
    "            if use_bn:\n",
    "                fc = tf.layers.batch_normalization(fc, training= is_training, name='fc_BN' + str(i), reuse= reuse)\n",
    "            fc = self.activation(fc)\n",
    "            inputs = fc \n",
    "        return inputs\n",
    "    def rnn_single_layers(self, inputs, is_training = True, reuse = False):\n",
    "        if is_training:\n",
    "            keep_probs = self.rnn_dropout_keep_prob\n",
    "        else:\n",
    "            keep_probs = np.ones_like(self.rnn_dropout_keep_prob)\n",
    "        cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_n_hiddens[0])\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probs[0])\n",
    "        # output_shape [batch_size, width(n_step), n_classes]\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        print(outputs.get_shape().as_list())\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        return outputs \n",
    "    \n",
    "    def rnn_multi_layers(self, inputs, is_training = True, reuse = False):\n",
    "        if is_training:\n",
    "            keep_probs = self.rnn_dropout_keep_prob\n",
    "            \n",
    "        else:\n",
    "            keep_probs = np.ones_like(self.rnn_dropout_keep_prob)\n",
    "        \n",
    "        cell1 = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_n_hiddens[0], reuse = reuse)\n",
    "        cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=keep_probs[0])\n",
    "        cell2 = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_n_hiddens[1], reuse = reuse)\n",
    "        cell2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=keep_probs[1])\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "        # output_shape [batch_size, width(n_step), n_classes]\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        print(outputs.get_shape().as_list())\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        return outputs\n",
    "    \n",
    "    def get_reshaped_cnn_to_rnn(self, inputs):\n",
    "        # [batch, height, width, n_feature map]\n",
    "        shape = inputs.get_shape().as_list() \n",
    "        # 우리가 얻어야하는 사이즈 [batch, height, width x n_feature map]\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, shape[1], shape[2] * shape[3]])\n",
    "        return reshaped_inputs\n",
    "    \n",
    "    # 모델 구축/ logit \n",
    "    def get_logits(self, X, is_training = True, reuse = False):        \n",
    "        conv = self.convolutional_layers(self.X, is_training, reuse)                           \n",
    "        #flat = tf.layers.flatten(conv)\n",
    "        reshaped_fp = self.get_reshaped_cnn_to_rnn(conv)\n",
    "        rnn = self.rnn_multi_layers(reshaped_fp, reuse)\n",
    "        #fc = self.fc_layers(flat, is_training, reuse)\n",
    "        output = tf.layers.dense(rnn, units= self.num_classes, reuse=reuse, name = 'out')\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # 모델 구축\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.logits_train = self.get_logits(self.X)                              \n",
    "            self.loss = tf.losses.softmax_cross_entropy(self.Y, self.logits_train)   \n",
    "                # batch_normalization 적용을 위해 모든 변수들을 불러와서 moving\n",
    "                #학습 단계에서는 데이터가 배치 단위로 들어오기 때문에 배치의 평균, 분산을 구하는 것이 가능하지만,\n",
    "                # 테스트 단계에서는 배치 단위로 평균/분산을 구하기가 어렵기때문에\n",
    "                # 학습 단계에서 배치 단위의 평균/분산을 저장해 놓고 테스트 시에는 평균/분산을 사용합니다.\n",
    "                # 저장한 값을 get_collection을 통해서 불러온다.\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)            \n",
    "            with tf.control_dependencies(update_ops):    \n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "            self.logits_eval = self.get_logits(self.X, is_training = False, reuse = True)\n",
    "            self.predict_proba_ = tf.nn.softmax(self.logits_eval)\n",
    "            self.prediction = tf.argmax(self.predict_proba_, 1)\n",
    "            self.accuracy = tf.metrics.accuracy(tf.argmax(self.Y, 1), self.prediction)\n",
    "            # 변수들 프린트/ 텐서보드 summary 생성\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('accuracy', self.accuracy[1])\n",
    "        for v in tf.trainable_variables():\n",
    "            tf.summary.histogram('Var_{}'.format(v.name), v)\n",
    "            print(v)            \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        # 모델저장\n",
    "        saver = tf.train.Saver()\n",
    "            \n",
    "        \n",
    "    def fit(self):\n",
    "        total_batch = int(mnist.train.num_examples/ self.batch_size)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
    "        for epoch in range(10):\n",
    "            total_cost = 0\n",
    "        \n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(self.batch_size)\n",
    "                _, c, _summ = sess.run([self.optimizer, self.loss, self.merged], feed_dict = {self.X:batch_xs.reshape(-1, 28, 28, 1), self.Y: batch_ys})\n",
    "                writer.add_summary(_summ, i)\n",
    "            acc = sess.run(self.accuracy, feed_dict = {self.X: mnist.test.images.reshape(-1, 28, 28, 1), self.Y: mnist.test.labels})\n",
    "            \n",
    "            print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameter\n",
    "params = {\n",
    "    'num_classes': 10,\n",
    "    'num_filters': [8, 16, 32],\n",
    "    'filter_size': [2, 2, 2],\n",
    "    'activation' : tf.nn.relu,\n",
    "    'cnn_batch_norm' : [False, False, False],\n",
    "    'fc_hidden_units': [256, 128],\n",
    "    'fc_batch_norm': [False, False],\n",
    "    \n",
    "    'rnn_n_step': 28, #width, time\n",
    "    'rnn_n_hiddens': [128],\n",
    "    'rnn_dropout_keep_prob': [0.5],\n",
    "    \n",
    "    'learning_rate': 0.01,\n",
    "    \n",
    "    'batch_size': 100\n",
    "}\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class Model(object):\n",
    "\n",
    "    def __init__(self, params, sess, name):\n",
    "        # 하이퍼파라미터\n",
    "        self.num_classes = params['num_classes']\n",
    "        self.num_filters = params['num_filters']\n",
    "        self.filter_sizes = params['filter_size']\n",
    "        self.cnn_batch_norm  = params['cnn_batch_norm']\n",
    "        \n",
    "        self.fc_hidden_units = params['fc_hidden_units']\n",
    "        self.fc_batch_norm = params['fc_batch_norm']\n",
    "        \n",
    "        self.rnn_n_hiddens = params['rnn_n_hiddens']\n",
    "        self.rnn_dropout_keep_prob = params['rnn_dropout_keep_prob']\n",
    "        \n",
    "        self.learning_rate = params['learning_rate']\n",
    "        self.activation = params['activation']\n",
    "        self.batch_size = params['batch_size']        \n",
    "        \n",
    "        self.idx_convolutional_layers = range(1, len(self.filter_sizes) + 1)\n",
    "        self.idx_fc_layers = range(1, len(self.fc_hidden_units) + 1)\n",
    "        self.idx_rnn_layers = range(1, len(self.rnn_n_hiddens) + 1)\n",
    "        self.name = name\n",
    "        # 플레이스홀더\n",
    "        self.X = tf.placeholder(tf.float32, [None, 28, 28, 1], name=\"input_x\")\n",
    "        self.Y = tf.placeholder(tf.float32, [None, self.num_classes], name=\"input_y\")\n",
    "        self.sess = sess\n",
    "        \n",
    "        self._build_net()\n",
    "\n",
    "    #  컨볼루션 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def convolutional_layers(self, X, is_training = True, reuse = False):\n",
    "        inputs = X\n",
    "        for i, num_filter, filter_size, use_bn in zip(self.idx_convolutional_layers, self.num_filters, self.filter_sizes, self.cnn_batch_norm):            \n",
    "            L = tf.layers.conv2d(inputs,\n",
    "                                 filters=num_filter,\n",
    "                                 kernel_size=filter_size,\n",
    "                                 strides=1,\n",
    "                                 padding='SAME',\n",
    "                                 name = 'CONV'+str(i),\n",
    "                                 reuse= reuse)\n",
    "            if use_bn:\n",
    "                L= tf.layers.batch_normalization(L, training= is_training, name='BN' + str(i), reuse= reuse)\n",
    "            L = self.activation(L)\n",
    "            L = tf.layers.max_pooling2d(L, pool_size = 2, strides = 2, padding = 'SAME')\n",
    "            inputs = L\n",
    "        return inputs\n",
    "    #  dense 레이어를 params에서 받은 파라미터를 따라 구축\n",
    "    def fc_layers(self, X, is_training = True, reuse = False):\n",
    "        inputs = X\n",
    "        for i, units, use_bn in zip(self.idx_fc_layers, self.fc_hidden_units, self.fc_batch_norm):\n",
    "            fc = tf.layers.dense(inputs,\n",
    "                                 units=units,\n",
    "                                 reuse=reuse,\n",
    "                                 name = 'FC' + str(i))\n",
    "            if use_bn:\n",
    "                fc = tf.layers.batch_normalization(fc, training= is_training, name='fc_BN' + str(i), reuse= reuse)\n",
    "            fc = self.activation(fc)\n",
    "            inputs = fc \n",
    "        return inputs\n",
    "    \n",
    "     # LSTM 레이어 \n",
    "    def rnn_layers(self, inputs, is_training = True, reuse = False):\n",
    "        if is_training:\n",
    "            keep_probs = self.rnn_dropout_keep_prob\n",
    "            \n",
    "        else:\n",
    "            keep_probs = np.ones_like(self.rnn_dropout_keep_prob)\n",
    "        # single layer\n",
    "        if len(self.idx_rnn_layers) == 1:\n",
    "            cell = tf.nn.rnn_cell.BasicLSTMCell(self.rnn_n_hiddens[0], reuse = reuse)\n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_probs[0])\n",
    "        # multi layer \n",
    "        else:\n",
    "            cell_list = []\n",
    "            for i, n_hidden, keep_prob in zip(self.idx_rnn_layers, self.rnn_n_hiddens, keep_probs):\n",
    "                cell_ = tf.nn.rnn_cell.BasicLSTMCell(n_hidden, reuse = reuse)\n",
    "                cell_ = tf.nn.rnn_cell.DropoutWrapper(cell_, output_keep_prob=keep_prob)\n",
    "                cell_list.append(cell_)\n",
    "            cell = tf.nn.rnn_cell.MultiRNNCell(cell_list)\n",
    "        # output_shape [batch_size, width(n_step), n_classes]\n",
    "        outputs, states = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "        print(outputs.get_shape().as_list())\n",
    "        outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "        outputs = outputs[-1]\n",
    "        return outputs\n",
    "    \n",
    "    def get_reshaped_cnn_to_rnn(self, inputs):\n",
    "        # [batch, height, width, n_feature map]\n",
    "        shape = inputs.get_shape().as_list() \n",
    "        # 우리가 얻어야하는 사이즈 [batch, height, width x n_feature map]\n",
    "        reshaped_inputs = tf.reshape(inputs, [-1, shape[1], shape[2] * shape[3]])\n",
    "        return reshaped_inputs\n",
    "    \n",
    "    # 모델 구축/ logit \n",
    "    def get_logits(self, X, is_training = True, reuse = False):        \n",
    "        conv = self.convolutional_layers(self.X, is_training, reuse)                           \n",
    "        #flat = tf.layers.flatten(conv)\n",
    "        reshaped_fp = self.get_reshaped_cnn_to_rnn(conv)\n",
    "        rnn = self.rnn_layers(reshaped_fp, is_training, reuse)\n",
    "        #fc = self.fc_layers(flat, is_training, reuse)\n",
    "        output = tf.layers.dense(rnn, units= self.num_classes, reuse=reuse, name = 'out')\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # 모델 구축\n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.logits_train = self.get_logits(self.X)                              \n",
    "            self.loss = tf.losses.softmax_cross_entropy(self.Y, self.logits_train)   \n",
    "                # batch_normalization 적용을 위해 모든 변수들을 불러와서 moving\n",
    "                #학습 단계에서는 데이터가 배치 단위로 들어오기 때문에 배치의 평균, 분산을 구하는 것이 가능하지만,\n",
    "                # 테스트 단계에서는 배치 단위로 평균/분산을 구하기가 어렵기때문에\n",
    "                # 학습 단계에서 배치 단위의 평균/분산을 저장해 놓고 테스트 시에는 평균/분산을 사용합니다.\n",
    "                # 저장한 값을 get_collection을 통해서 불러온다.\n",
    "            self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=self.name)            \n",
    "            with tf.control_dependencies(update_ops):    \n",
    "                self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss, global_step=self.global_step)\n",
    "\n",
    "            self.logits_eval = self.get_logits(self.X, is_training = False, reuse = True)\n",
    "            self.predict_proba_ = tf.nn.softmax(self.logits_eval)\n",
    "            self.prediction = tf.argmax(self.predict_proba_, 1)\n",
    "            self.accuracy = tf.metrics.accuracy(tf.argmax(self.Y, 1), self.prediction)\n",
    "            # 변수들 프린트/ 텐서보드 summary 생성\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('accuracy', self.accuracy[1])\n",
    "        for v in tf.trainable_variables():\n",
    "            tf.summary.histogram('Var_{}'.format(v.name), v)\n",
    "            print(v)            \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        # 모델저장\n",
    "        saver = tf.train.Saver()\n",
    "            \n",
    "        \n",
    "    def fit(self):\n",
    "        total_batch = int(mnist.train.num_examples/ self.batch_size)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(tf.local_variables_initializer())\n",
    "        writer = tf.summary.FileWriter('./logs/', sess.graph)\n",
    "        for epoch in range(10):\n",
    "            total_cost = 0\n",
    "        \n",
    "            for i in range(total_batch):\n",
    "                batch_xs, batch_ys = mnist.train.next_batch(self.batch_size)\n",
    "                _, c, _summ = sess.run([self.optimizer, self.loss, self.merged], feed_dict = {self.X:batch_xs.reshape(-1, 28, 28, 1), self.Y: batch_ys})\n",
    "                writer.add_summary(_summ, i)\n",
    "            acc = sess.run(self.accuracy, feed_dict = {self.X: mnist.test.images.reshape(-1, 28, 28, 1), self.Y: mnist.test.labels})\n",
    "            \n",
    "            print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 4, 128]\n",
      "[None, 4, 128]\n",
      "INFO:tensorflow:Summary name Var_model/CONV1/kernel:0 is illegal; using Var_model/CONV1/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV1/kernel:0' shape=(2, 2, 1, 8) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV1/bias:0 is illegal; using Var_model/CONV1/bias_0 instead.\n",
      "<tf.Variable 'model/CONV1/bias:0' shape=(8,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV2/kernel:0 is illegal; using Var_model/CONV2/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV2/kernel:0' shape=(2, 2, 8, 16) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV2/bias:0 is illegal; using Var_model/CONV2/bias_0 instead.\n",
      "<tf.Variable 'model/CONV2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV3/kernel:0 is illegal; using Var_model/CONV3/kernel_0 instead.\n",
      "<tf.Variable 'model/CONV3/kernel:0' shape=(2, 2, 16, 32) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/CONV3/bias:0 is illegal; using Var_model/CONV3/bias_0 instead.\n",
      "<tf.Variable 'model/CONV3/bias:0' shape=(32,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/basic_lstm_cell/kernel:0 is illegal; using Var_model/rnn/basic_lstm_cell/kernel_0 instead.\n",
      "<tf.Variable 'model/rnn/basic_lstm_cell/kernel:0' shape=(256, 512) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/rnn/basic_lstm_cell/bias:0 is illegal; using Var_model/rnn/basic_lstm_cell/bias_0 instead.\n",
      "<tf.Variable 'model/rnn/basic_lstm_cell/bias:0' shape=(512,) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/out/kernel:0 is illegal; using Var_model/out/kernel_0 instead.\n",
      "<tf.Variable 'model/out/kernel:0' shape=(128, 10) dtype=float32_ref>\n",
      "INFO:tensorflow:Summary name Var_model/out/bias:0 is illegal; using Var_model/out/bias_0 instead.\n",
      "<tf.Variable 'model/out/bias:0' shape=(10,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "model = Model(params, sess, name = 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, cost : 0.11908344179391861, acc: (0.90190911, 0.91336924)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-fed1f18ffa7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-82-2c84b9ff6077>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_summ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_summ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\POWER USER\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, cost : 0.053370747715234756, acc: [(0.0, 0.96810001)]\n",
      "epoch : 1, cost : 0.06267564743757248, acc: [(0.96810001, 0.97430003)]\n",
      "epoch : 2, cost : 0.0855538621544838, acc: [(0.97430003, 0.97680002)]\n",
      "epoch : 3, cost : 0.19877442717552185, acc: [(0.97680002, 0.97907501)]\n",
      "epoch : 4, cost : 0.05799204856157303, acc: [(0.97907501, 0.98093998)]\n",
      "epoch : 5, cost : 0.11063935607671738, acc: [(0.98093998, 0.98213333)]\n",
      "epoch : 6, cost : 0.03810318559408188, acc: [(0.98213333, 0.98282856)]\n",
      "epoch : 7, cost : 0.0006602299981750548, acc: [(0.98282856, 0.98348749)]\n",
      "epoch : 8, cost : 0.033298783004283905, acc: [(0.98348749, 0.98413336)]\n",
      "epoch : 9, cost : 0.008241044357419014, acc: [(0.98413336, 0.98462999)]\n",
      "epoch : 10, cost : 0.02387131005525589, acc: [(0.98462999, 0.98500907)]\n",
      "epoch : 11, cost : 0.0030551606323570013, acc: [(0.98500907, 0.98540002)]\n",
      "epoch : 12, cost : 0.06204137206077576, acc: [(0.98540002, 0.98581541)]\n",
      "epoch : 13, cost : 0.0004125334962736815, acc: [(0.98581541, 0.98612142)]\n",
      "epoch : 14, cost : 0.023427624255418777, acc: [(0.98612142, 0.98637998)]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev = 0.01))\n",
    "L1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L1 = tf.nn.relu(L1)\n",
    "L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev = 0.01))\n",
    "L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
    "L2 = tf.nn.relu(L2)\n",
    "L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "L2_flatten = tf.contrib.layers.flatten(L2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([7 * 7 * 64, 256], stddev = 0.01))\n",
    "b3 = tf.Variable(tf.random_normal([256]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2_flatten, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob)\n",
    "\n",
    "W4 = tf.Variable(tf.random_normal([256, 10], stddev =0.01))\n",
    "b4 = tf.Variable(tf.random_normal([10]))\n",
    "logits = tf.matmul(L3, W4) + b4\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(Y, logits = logits)\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "predictions = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), predictions)\n",
    "\n",
    "batch_size = 100\n",
    "with tf.Session() as sess:\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for epoch in range(15):\n",
    "        total_cost = 0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c, = sess.run([train, cost], feed_dict = {X:batch_xs.reshape(-1, 28, 28, 1), Y: batch_ys, keep_prob: 0.8})\n",
    "        acc = sess.run([accuracy], feed_dict = {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels, keep_prob: 1.0})\n",
    "        print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def model_net(x, activation, is_training, reuse = False):\n",
    "    L1 = tf.layers.conv2d(x, 32, 3, padding='SAME', activation = activation, reuse= reuse, name = 'L1')\n",
    "    L1 = tf.layers.max_pooling2d(L1, 2, 2)\n",
    "    L2 = tf.layers.conv2d(L1, 64, 3, padding='SAME', activation = activation, reuse=reuse, name = 'L2')\n",
    "    L2 = tf.layers.max_pooling2d(L2, 2, 2)\n",
    "    \n",
    "    L2_flatten = tf.contrib.layers.flatten(L2)\n",
    "    \n",
    "    fc1 = tf.layers.dense(L2_flatten, 256, activation = activation, reuse=reuse, name = 'FC1')\n",
    "    fc1 = tf.layers.dropout(fc1, 0.2, training = is_training)\n",
    "    \n",
    "    fc2 = tf.layers.dense(fc1, 10, reuse=reuse, name = 'output')\n",
    "    return fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = model_net(X, tf.nn.relu, True)\n",
    "test_logits = model_net(X, tf.nn.relu, False, True)\n",
    "cost = tf.losses.softmax_cross_entropy(Y, logits = logits)\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "predictions = tf.argmax(tf.nn.softmax(test_logits), 1)\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, cost : 0.020873257890343666, acc: [(0.0, 0.98430002)]\n",
      "epoch : 1, cost : 0.012488684616982937, acc: [(0.98430002, 0.98689997)]\n",
      "epoch : 2, cost : 0.058932267129421234, acc: [(0.98689997, 0.9878)]\n",
      "epoch : 3, cost : 0.050051361322402954, acc: [(0.9878, 0.98860002)]\n",
      "epoch : 4, cost : 0.047780223190784454, acc: [(0.98860002, 0.98923999)]\n",
      "epoch : 5, cost : 0.03912124037742615, acc: [(0.98923999, 0.98943335)]\n",
      "epoch : 6, cost : 0.02453668788075447, acc: [(0.98943335, 0.98968571)]\n",
      "epoch : 7, cost : 0.0036200848408043385, acc: [(0.98968571, 0.98982501)]\n",
      "epoch : 8, cost : 0.0011123416479676962, acc: [(0.98982501, 0.99014443)]\n",
      "epoch : 9, cost : 0.004260535817593336, acc: [(0.99014443, 0.98979002)]\n",
      "epoch : 10, cost : 0.007146528456360102, acc: [(0.98979002, 0.98985457)]\n",
      "epoch : 11, cost : 0.04281054064631462, acc: [(0.98985457, 0.99010831)]\n",
      "epoch : 12, cost : 0.0008145206375047565, acc: [(0.99010831, 0.99017692)]\n",
      "epoch : 13, cost : 0.02385837584733963, acc: [(0.99017692, 0.99017859)]\n",
      "epoch : 14, cost : 0.00028013234259560704, acc: [(0.99017859, 0.99027997)]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "with tf.Session() as sess:\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for epoch in range(15):\n",
    "        total_cost = 0\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c, = sess.run([train, cost], feed_dict = {X:batch_xs.reshape(-1, 28, 28, 1), Y: batch_ys})\n",
    "        acc = sess.run([accuracy], feed_dict = {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "        print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))\n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "def model_net(x, activation, is_training, reuse = False):\n",
    "    L1 = tf.layers.conv2d(x, 32, 3, padding='SAME', reuse= reuse, name = 'L1')\n",
    "    L1 = tf.layers.batch_normalization(L1, training=is_training)\n",
    "    L1 = activation(L1)\n",
    "    L1 = tf.layers.max_pooling2d(L1, 2, 2)\n",
    "    \n",
    "    L2 = tf.layers.conv2d(L1, 64, 3, padding='SAME', activation = activation, reuse=reuse, name = 'L2')\n",
    "    L2 = tf.layers.batch_normalization(L2, training=is_training)\n",
    "    L2 = activation(L2)\n",
    "    L2 = tf.layers.max_pooling2d(L2, 2, 2)\n",
    "    \n",
    "    L2_flatten = tf.contrib.layers.flatten(L2)\n",
    "    \n",
    "    fc1 = tf.layers.dense(L2_flatten, 256, activation = activation, reuse=reuse, name = 'FC1')\n",
    "    fc1 = tf.layers.dropout(fc1, 0.2, training = is_training)\n",
    "    \n",
    "    fc2 = tf.layers.dense(fc1, 10, reuse=reuse, name = 'output')\n",
    "    return fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logits = model_net(X, tf.nn.relu, True)\n",
    "test_logits = model_net(X, tf.nn.relu, False, True)\n",
    "cost = tf.losses.softmax_cross_entropy(Y, logits = logits)\n",
    "train = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "predictions = tf.argmax(tf.nn.softmax(test_logits), 1)\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(Y, 1), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "with tf.Session() as sess:\n",
    "    total_batch = int(mnist.train.num_examples/ batch_size)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    for epoch in range(15):\n",
    "        total_cost = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c, = sess.run([train, cost], feed_dict = {X:batch_xs.reshape(-1, 28, 28, 1), Y: batch_ys})\n",
    "        acc = sess.run([accuracy], feed_dict = {X: mnist.test.images.reshape(-1, 28, 28, 1), Y: mnist.test.labels})\n",
    "        print('epoch : {}, cost : {}, acc: {}'.format(epoch, c, acc))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
